{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MEPP Walkthrough: Motif Enrichment Positional Profiling\n",
    "This notebook will walk you through how to perform a score based motif enrichment analysis that profiles motif enrichment at multiple positions across a set of sequences centered on biologically relevant features, in this case a central binding motif of interest. \n",
    "\n",
    "## Quickstart\n",
    "If you already have MEPP installed, have a scored BED file, and want to quickly get started, jump to the section [\"Convert scored bed file to scored sequences, and run MEPP analysis\"](#Convert-scored-bed-file-to-scored-sequences,-and-run-MEPP-analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install prerequisites\n",
    "\n",
    "You will need the following prerequisites:\n",
    "* MEPP\n",
    "* HOMER\n",
    "* pandas\n",
    "* numpy\n",
    "* gtfparse\n",
    "* coolbox\n",
    "* wget\n",
    "* samtools\n",
    "* deeptools\n",
    "* bedtools\n",
    "* bedops\n",
    "* wiggletools\n",
    "* wigToBigWig\n",
    "* bigWigToWig\n",
    "* tensorflow\n",
    "\n",
    "To install most of these through mamba:\n",
    "```\n",
    "mamba create -d -n mepp_walkthrough -c bioconda -c conda-forge homer pandas numpy gtfparse coolbox wget samtools deeptools bedtools bedops wiggletools ucsc-wigtobigwig ucsc-bigwigtowig tensorflow\n",
    "```\n",
    "\n",
    "To install most of these through conda (slower):\n",
    "```\n",
    "conda create -d -n mepp_walkthrough -c bioconda -c conda-forge homer pandas numpy coolbox wget samtools deeptools bedtools bedops wiggletools ucsc-wigtobigwig ucsc-bigwigtowig tensorflow\n",
    "```\n",
    "\n",
    "To activate the environment:\n",
    "```\n",
    "conda activate mepp_walkthrough\n",
    "```\n",
    "\n",
    "To install MEPP, use pip:\n",
    "```\n",
    "pip install git+https://github.com/npdeloss/mepp@main\n",
    "```\n",
    "\n",
    "Or, if you only have user privileges:\n",
    "```\n",
    "pip install git+https://github.com/npdeloss/mepp@main --user\n",
    "```\n",
    "\n",
    "You may need to append the following to your ~/.bashrc:\n",
    "```\n",
    "export PATH=\"$HOME/.local/bin:$PATH\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import key libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enumerate sample sheet with alignment files for download\n",
    "Here we will be comparing K562 and HCT116 cell lines from ENCODE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file k562.vs.hct116.atac-seq.samples.txt\n",
    "cell_type replicate bam_url\n",
    "k562 1 https://www.encodeproject.org/files/ENCFF512VEZ/@@download/ENCFF512VEZ.bam\n",
    "k562 2 https://www.encodeproject.org/files/ENCFF987XOV/@@download/ENCFF987XOV.bam\n",
    "hct116 1 https://www.encodeproject.org/files/ENCFF724QHH/@@download/ENCFF724QHH.bam\n",
    "hct116 2 https://www.encodeproject.org/files/ENCFF927YUB/@@download/ENCFF927YUB.bam\n",
    "hepg2 1 https://www.encodeproject.org/files/ENCFF239RGZ/@@download/ENCFF239RGZ.bam\n",
    "hepg2 2 https://www.encodeproject.org/files/ENCFF394BBD/@@download/ENCFF394BBD.bam\n",
    "dnd-41 1 https://www.encodeproject.org/files/ENCFF538YYI/@@download/ENCFF538YYI.bam\n",
    "dnd-41 2 https://www.encodeproject.org/files/ENCFF080WSN/@@download/ENCFF080WSN.bam\n",
    "dnd-41 3 https://www.encodeproject.org/files/ENCFF626KDS/@@download/ENCFF626KDS.bam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load sample sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samplesheet_filepath = 'k562.vs.hct116.atac-seq.samples.txt'\n",
    "samplesheet_sep = ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samplesheet_df = pd.read_csv(samplesheet_filepath, sep = samplesheet_sep)\n",
    "samplesheet_df['sample'] = True\n",
    "samplesheet_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Alignment files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samplesheet_df['basename'] = samplesheet_df['cell_type'] + '_rep' + samplesheet_df['replicate'].astype(str)\n",
    "samplesheet_df['bam_filepath'] = samplesheet_df['basename'] + '.bam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samplesheet_df['wget_bam_cmd'] = (\n",
    "    'wget -nc -O ' +\n",
    "    samplesheet_df['bam_filepath'] + ' ' +\n",
    "    '\"' + samplesheet_df['bam_url'] + '\"'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "run_wget_bam_cmds = run_cmd = True\n",
    "for cmd in list(samplesheet_df['wget_bam_cmd']):\n",
    "    print(cmd)\n",
    "    if run_cmd:\n",
    "        ! {cmd}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index alignment files\n",
    "Necessary to generate bigWig files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_bam_threads = 8\n",
    "samplesheet_df['index_bam_cmd'] = (\n",
    "    'samtools index ' +\n",
    "    samplesheet_df['bam_filepath']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "run_index_bam_cmds = run_cmd = True\n",
    "for cmd in list(samplesheet_df['index_bam_cmd']):\n",
    "    print(cmd)\n",
    "    if run_cmd:\n",
    "        ! {cmd}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute coverage bigWig files from alignment files\n",
    "You will need to use [effective genome size](https://deeptools.readthedocs.io/en/latest/content/feature/effectiveGenomeSize.html) numbers for the relevant genome.\n",
    "You could also use other normalizations of choice at this step. Here we use the deeptools `bamCoverage` defaults.\n",
    "These will allow you to later visualize and quantify calculations on this coverage data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "effective_genome_size = 2913022398\n",
    "bamcoverage_binsize = 10\n",
    "bamcoverage_threads = 'max/2'\n",
    "# Value for GRCh38, from:\n",
    "# https://deeptools.readthedocs.io/en/latest/content/feature/effectiveGenomeSize.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samplesheet_df['bw_filepath'] = samplesheet_df['basename'] + '.bw'\n",
    "samplesheet_df['raw_bw_filepath'] = samplesheet_df['basename'] + '.raw.bw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samplesheet_df['bamcoverage_cmd'] = (\n",
    "    f'bamCoverage ' + \n",
    "    f' -p {bamcoverage_threads} ' + \n",
    "    f' --effectiveGenomeSize {effective_genome_size} ' + \n",
    "    f' --normalizeUsing RPKM '\n",
    "    f' -bs {bamcoverage_binsize}'\n",
    "    f' -b ' + samplesheet_df['bam_filepath'] + \n",
    "    f' -o ' + samplesheet_df['bw_filepath']\n",
    ")\n",
    "\n",
    "samplesheet_df['bamcoverage_raw_cmd'] = (\n",
    "    f'bamCoverage ' + \n",
    "    f' -p {bamcoverage_threads} ' + \n",
    "    f' --effectiveGenomeSize {effective_genome_size} ' + \n",
    "    f' --normalizeUsing None '\n",
    "    f' -bs {bamcoverage_binsize}'\n",
    "    f' -b ' + samplesheet_df['bam_filepath'] + \n",
    "    f' -o ' + samplesheet_df['raw_bw_filepath']\n",
    ")\n",
    "\n",
    "bamcoverage_cmds = list(samplesheet_df['bamcoverage_cmd']) + list(samplesheet_df['bamcoverage_raw_cmd'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "run_bamcoverage_cmds = run_cmd = True\n",
    "for cmd in bamcoverage_cmds:\n",
    "    print(cmd)\n",
    "    if run_cmd:\n",
    "        ! {cmd}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download reference genome\n",
    "Also generate index and chromosome size files for bedtools and wigToBigWig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genome_fa_filepath = 'hg38.fa'\n",
    "genome_fai_filepath = f'{genome_fa_filepath}.fai'\n",
    "genome_chromsizes_filepath = f'{genome_fa_filepath}.chromsizes.tab'\n",
    "genome_fa_gz_url = 'https://hgdownload.soe.ucsc.edu/goldenPath/hg38/bigZips/latest/hg38.fa.masked.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_genome_fa_cmd = f'wget -nc -O {genome_fa_filepath}.gz \"{genome_fa_gz_url}\"; zcat {genome_fa_filepath}.gz > {genome_fa_filepath}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_genome_fa_cmd = f'samtools faidx {genome_fa_filepath}'\n",
    "genome_chromsizes_cmd = f'cut -f1,2 {genome_fai_filepath} > {genome_chromsizes_filepath}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "run_download_genome_fa_cmd = run_cmd = True\n",
    "for cmd in [download_genome_fa_cmd, index_genome_fa_cmd, genome_chromsizes_cmd]:\n",
    "    print(cmd)\n",
    "    if run_cmd:\n",
    "        ! {cmd}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Designate comparison\n",
    "Here we compare HCT116 vs. K562 cells. Comparison groups are designated by the `cell_type` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_1  = 'hct116'\n",
    "group_2  = 'k562'\n",
    "\n",
    "group_column = 'cell_type'\n",
    "sample_column = 'sample'\n",
    "sort_column = 'replicate'\n",
    "\n",
    "comparison_prefix = f'{group_1}.vs.{group_2}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(comparison_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List bigwigs belonging to each group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_subset_df = samplesheet_df[samplesheet_df[sample_column]].sort_values(by = sort_column).copy()\n",
    "\n",
    "group_1_sample_bw_filepaths = list(sample_subset_df[sample_subset_df[group_column] == group_1]['bw_filepath'])\n",
    "group_2_sample_bw_filepaths = list(sample_subset_df[sample_subset_df[group_column] == group_2]['bw_filepath'])\n",
    "sample_bw_filepaths = group_1_sample_bw_filepaths + group_2_sample_bw_filepaths\n",
    "group_1_sample_bw_filepaths_str = ' '.join(group_1_sample_bw_filepaths)\n",
    "group_2_sample_bw_filepaths_str = ' '.join(group_2_sample_bw_filepaths)\n",
    "sample_bw_filepaths_str = ' '.join(sample_bw_filepaths)\n",
    "\n",
    "group_1_sample_raw_bw_filepaths = list(sample_subset_df[sample_subset_df[group_column] == group_1]['raw_bw_filepath'])\n",
    "group_2_sample_raw_bw_filepaths = list(sample_subset_df[sample_subset_df[group_column] == group_2]['raw_bw_filepath'])\n",
    "sample_raw_bw_filepaths = group_1_sample_raw_bw_filepaths + group_2_sample_raw_bw_filepaths\n",
    "group_1_sample_raw_bw_filepaths_str = ' '.join(group_1_sample_raw_bw_filepaths)\n",
    "group_2_sample_raw_bw_filepaths_str = ' '.join(group_2_sample_raw_bw_filepaths)\n",
    "sample_raw_bw_filepaths_str = ' '.join(sample_raw_bw_filepaths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Calculate Bigwig of Log2FC between groups.\n",
    "First we calculate the means of each group, in `{group_1}.mean.bw` and `{group_2}.mean.bw`. Then we compute the Log2 Fold Change (with pseudocount) as `log2((group_1_mean+1)/(group_2_mean+1))`. A pseudocount prevents division by zero in the ratio calculation.\n",
    "\n",
    "We also compute the sum of coverage across all samples, for use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_1_bw_filepath = f'{group_1}.mean.bw'\n",
    "group_2_bw_filepath = f'{group_2}.mean.bw'\n",
    "\n",
    "sum_bw_filepath = f'{group_1}.vs.{group_2}.sum.bw'\n",
    "log2fc_bw_filepath = f'{group_1}.vs.{group_2}.log2fc.bw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_1_bw_cmd = (\n",
    "    f'wiggletools write {group_1_bw_filepath}.wig mean {group_1_sample_bw_filepaths_str} ; '\n",
    "    f'wigToBigWig -clip {group_1_bw_filepath}.wig {genome_chromsizes_filepath} {group_1_bw_filepath} ; '\n",
    "    f'rm {group_1_bw_filepath}.wig'\n",
    ")\n",
    "group_2_bw_cmd = (\n",
    "    f'wiggletools write {group_2_bw_filepath}.wig mean {group_2_sample_bw_filepaths_str} ; '\n",
    "    f'wigToBigWig -clip {group_2_bw_filepath}.wig {genome_chromsizes_filepath} {group_2_bw_filepath} ; '\n",
    "    f'rm {group_2_bw_filepath}.wig'\n",
    ")\n",
    "\n",
    "sum_bw_cmd = (\n",
    "    f'wiggletools write {sum_bw_filepath}.wig sum {group_1_sample_raw_bw_filepaths_str} {group_2_sample_raw_bw_filepaths_str} ; '\n",
    "    f'wigToBigWig -clip {sum_bw_filepath}.wig {genome_chromsizes_filepath} {sum_bw_filepath} ; '\n",
    "    f'rm {sum_bw_filepath}.wig'\n",
    ")\n",
    "\n",
    "log2fc_bw_cmd = (\n",
    "    f'wiggletools write {group_1_bw_filepath}.plus_1.wig offset 1 {group_1_bw_filepath} ; '\n",
    "    f'wiggletools write {group_2_bw_filepath}.plus_1.wig offset 1 {group_2_bw_filepath} ; '\n",
    "    f'wiggletools write {group_1}.vs.{group_2}.ratio.wig ratio {group_1_bw_filepath}.plus_1.wig {group_2_bw_filepath}.plus_1.wig ; '\n",
    "    f'wiggletools write {log2fc_bw_filepath}.wig log 2 {group_1}.vs.{group_2}.ratio.wig ; '\n",
    "    f'wigToBigWig -clip {log2fc_bw_filepath}.wig {genome_chromsizes_filepath} {log2fc_bw_filepath} ; '\n",
    "    f'rm {group_1_bw_filepath}.plus_1.wig ; '\n",
    "    f'rm {group_2_bw_filepath}.plus_1.wig ; '\n",
    "    f'rm {group_1}.vs.{group_2}.ratio.wig ; '\n",
    "    f'rm {log2fc_bw_filepath}.wig '\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bw_cmds = [group_1_bw_cmd, group_2_bw_cmd, sum_bw_cmd, log2fc_bw_cmd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "run_bw_cmds = run_cmd = True\n",
    "for cmd in list(bw_cmds):\n",
    "    print(cmd)\n",
    "    if run_cmd:\n",
    "        ! {cmd}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download annotation file\n",
    "Not strictly necessary, just for the benefit of visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# genes_gtf_filepath = 'hg38_ensGene.gtf'\n",
    "# genes_gtf_gz_url = 'https://hgdownload.soe.ucsc.edu/goldenPath/hg38/bigZips/genes/hg38.ensGene.gtf.gz'\n",
    "\n",
    "genes_gtf_filepath = 'ENCFF159KBI.gtf'\n",
    "genes_bed_filepath = genes_gtf_filepath[:-len('.gtf')]+'.bed'\n",
    "genes_gtf_gz_url = 'https://www.encodeproject.org/files/ENCFF159KBI/@@download/ENCFF159KBI.gtf.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_genes_gtf_cmd = f'wget -nc -O {genes_gtf_filepath}.gz \"{genes_gtf_gz_url}\"; zcat {genes_gtf_filepath}.gz > {genes_gtf_filepath}'\n",
    "# index_gtf_cmd = f'tabix -p gff {genes_gtf_filepath}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "run_download_genes_gtf_cmd = run_cmd = True\n",
    "for cmd in [download_genes_gtf_cmd]:\n",
    "    print(cmd)\n",
    "    if run_cmd:\n",
    "        ! {cmd}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head {genes_gtf_filepath}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtfparse import read_gtf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genes_df = read_gtf(genes_gtf_filepath)\n",
    "genes_df = genes_df[genes_df[\"feature\"] == \"gene\"].copy()\n",
    "genes_df = genes_df[['seqname', 'start', 'end', 'gene_name', 'score', 'strand']].copy()\n",
    "genes_df['start'] = genes_df['start']-1\n",
    "genes_df['score'] = 0\n",
    "genes_df.to_csv(genes_bed_filepath, sep = '\\t', index = False, header = None)\n",
    "genes_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Log2 Fold Change bigWig with Coolbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import coolbox\n",
    "from coolbox.api import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coolbox.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genes_df[genes_df['gene_name']=='HBB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_margin = 2000\n",
    "test_gene_df=genes_df[genes_df['gene_name']=='HBB'].copy().reset_index(drop = True)\n",
    "test_chr = list(test_gene_df['seqname'])[0]\n",
    "test_start = list(test_gene_df['start'])[0] - test_margin\n",
    "test_end = list(test_gene_df['end'])[0] + test_margin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_range = 'chr9:5000000-5500000'\n",
    "test_range = f'{test_chr}:{test_start}-{test_end}'\n",
    "frame = (\n",
    "    XAxis() + \n",
    "    BED(genes_bed_filepath) +\n",
    "    Title('Genes') + TrackHeight(8) + Color('#323232') +\n",
    "    BigWig(log2fc_bw_filepath) + Title('Log2FC') + Color('#cf32cf') +\n",
    "    BigWig(group_1_bw_filepath) + Title(group_1) + Color('#3232cf') +\n",
    "    BigWig(group_2_bw_filepath) + Title(group_2) + Color('#cf3232') +\n",
    "    BigWig(sum_bw_filepath) + Title('Coverage') + Color('#32cd32')\n",
    ")\n",
    "frame.plot(test_range)\n",
    "# bsr = Browser(frame)\n",
    "# bsr.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copy over the HOMER motif library\n",
    "Use the below commands to locate and copy your motif library if you installed HOMER with conda/mamba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_homer_motif_cmd = f'cp -rf $(dirname $(which homer))/../share/homer/motifs ./homer_motifs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "run_copy_homer_motif_cmd = run_cmd = True\n",
    "for cmd in [copy_homer_motif_cmd]:\n",
    "    print(cmd)\n",
    "    if run_cmd:\n",
    "        ! {cmd}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scan for motif instances to center sequence on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motif_basename = 'gata'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motif_subpath = '/'.join(motif_basename.split(' '))\n",
    "motif_safe_basename = '_'.join(motif_subpath.split('/'))\n",
    "scanned_motif_filepath = f'homer_motifs/{motif_subpath}.motif'\n",
    "motif_scans_bed_filepath = f'{motif_safe_basename}.scans.bed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan_motifs_cmd = (\n",
    "    f'scanMotifGenomeWide.pl {scanned_motif_filepath} {genome_fa_filepath} '\n",
    "    f'-bed -5p 1> {motif_scans_bed_filepath} 2> {motif_scans_bed_filepath}.log'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "run_scan_motifs_cmd = run_cmd = True\n",
    "for cmd in [scan_motifs_cmd]:\n",
    "    print(cmd)\n",
    "    if run_cmd:\n",
    "        ! {cmd}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head {scanned_motif_filepath}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head {motif_scans_bed_filepath}\n",
    "! wc -l {motif_scans_bed_filepath}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score motif scans by Log2 Fold Change, and annotate with coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scored_motif_scans_filepath = f'{motif_safe_basename}.scans.scored_by.{log2fc_bw_filepath}.bed'\n",
    "coverage_summed_motif_scans_filepath = f'{motif_safe_basename}.scans.scored_by.{sum_bw_filepath}.bed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_scans_cmd = f'bigWigToWig {log2fc_bw_filepath} >(wig2bed -x) | bedmap --echo --delim \\'\\\\t\\' --wmean {motif_scans_bed_filepath} - | awk \\'$7!=\"NAN\"\\' | awk \\'{{FS=OFS=\"\\\\t\";$5=$7;print $1,$2,$3,$4,$5,$6}}\\' > {scored_motif_scans_filepath}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_sum_scans_cmd = f'bigWigToWig {sum_bw_filepath} >(wig2bed -x) | bedmap --echo --delim \\'\\\\t\\' --wmean {scored_motif_scans_filepath} - > {coverage_summed_motif_scans_filepath}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "run_score_scans_cmds = run_cmd = True\n",
    "for cmd in [score_scans_cmd, coverage_sum_scans_cmd]:\n",
    "    print(cmd)\n",
    "    if run_cmd:\n",
    "        ! {cmd}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head {scored_motif_scans_filepath}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head {coverage_summed_motif_scans_filepath}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform cluster deduplication, designate sequence length\n",
    "By default, HOMER genomewide motif scans extract intervals +/-100bp of the motif 5' end. This step prevents extraction of overlapping sequences in this interval, which prevents e.g. identification of artifactual periodicities/positionalities due to repetitive sequence. For example, you might have a motif repeat with itself within +/- 100bp of its own instances, giving rise to artificial periodicity due to repetitive sampling of the same genomic DNA.\n",
    "\n",
    "Briefly, we cluster overlapping intervals, then for each cluster we select only the interval with the highest summed coverage across all samples.\n",
    "\n",
    "We also only select intervals witha minimum summed coverage of 5, to avoid picking up unbound intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = max(sequence_length, 200)\n",
    "slop = (sequence_length-200)//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_deduplicated_scored_motif_scans_filepath = scored_motif_scans_filepath[:-len('.bed')] + '.cluster_deduplicated.bed'\n",
    "slopped_cluster_deduplicated_scored_motif_scans_filepath = cluster_deduplicated_scored_motif_scans_filepath[:-len('.bed')] + f'.slop_{slop}.bed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_coverage = 5.0\n",
    "cluster_deduplication_cmd = (\n",
    "    f'bedtools cluster -s -i {coverage_summed_motif_scans_filepath} '\n",
    "    f'|awk \\'$7>={min_coverage}\\' '\n",
    "    f'| sort -k8,8n -k7,7nr | awk \\'!a[$8]++\\' '\n",
    "    f'| bedtools sort -i - |cut -f1-6 '\n",
    "    f'> {cluster_deduplicated_scored_motif_scans_filepath}'\n",
    ")\n",
    "\n",
    "slop_cmd = (\n",
    "    f'bedtools slop -i {cluster_deduplicated_scored_motif_scans_filepath} -b {slop} -g {genome_chromsizes_filepath} > {slopped_cluster_deduplicated_scored_motif_scans_filepath}'\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "run_bedtools_cmds = run_cmd = True\n",
    "for cmd in [cluster_deduplication_cmd, slop_cmd]:\n",
    "    print(cmd)\n",
    "    if run_cmd:\n",
    "        ! {cmd}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head {slopped_cluster_deduplicated_scored_motif_scans_filepath}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preview BED file of scored intervals "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bed_columns = 'Chr Start End Name Score Strand'.split()\n",
    "bed_df = pd.read_csv(cluster_deduplicated_scored_motif_scans_filepath, sep = '\\t', header = None, names = bed_columns)\n",
    "bed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize score distribution\n",
    "Like most tools, MEPP prefers normal score distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bed_df[['Score']].hist(bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download JASPAR-converted HOMER vertebrate motifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motifs_url = 'https://raw.githubusercontent.com/npdeloss/mepp/main/data/homer.motifs.txt'\n",
    "motifs_filepath = 'homer.motifs.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wget_motifs_cmd = (\n",
    "    f'wget -nc -O {motifs_filepath} \"{motifs_url}\"'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "run_wget_motifs_cmds = run_cmd = True\n",
    "for cmd in [wget_motifs_cmd]:\n",
    "    print(cmd)\n",
    "    if run_cmd:\n",
    "        ! {cmd}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert scored bed file to scored sequences, and run MEPP analysis\n",
    "In `mepp.get_scored_fasta` we handle reverse complementation of sequence according the BED interval's strand value. We can then pipe that directly into MEPP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation of parameters\n",
    "\n",
    "`python -m mepp.get_scored_fasta`\n",
    "* Utility for extracting scored FASTA files (sequence score in header) from scored bed files\n",
    "    * `-fi {genome_fa_filepath}`: Extract sequence from the specified genome FASTA (required).\n",
    "    * `-bed {bed_filepath}`: Extract sequences from the intervals specified in this BED file (required). \n",
    "    \n",
    "`|python -m mepp.cli`\n",
    "* Pipe output from previous command into MEPP\n",
    "    * `--fa - `: Receive scored FASTA from the output of the previous command (Required).\n",
    "    * `--motifs {motifs_filepath}:` Analyze these motifs from a JASPAR-formatted motif matrix collection file (Required).\n",
    "    * `--out {mepp_filepath} `: Output to this directory (Required).\n",
    "    * `--perms 100 `: Use 100 permutations for confidence interval statistics (Default: 1000, can be costly in time & memory).\n",
    "    * `--batch 1000`: Use 1000 as tensorflow batch size (Default: 1000, adjust according to machine memory)\n",
    "    * `dgt 50`: Only analyze sequences with less than 50% degenerate base content (Default: 100, adjust according to analysis needs)\n",
    "    * `--jobs 20`: Use 20 jobs for multithreaded tasks. (Default: Use all cores)\n",
    "    * `--gjobs 20`: Use 20 jobs for multithreaded tasks optimizable by Tensorflow GPU usage. (Default: 1)\n",
    "    * `--nogpu`: Don't use the GPU (Default: Use GPU.)\n",
    "        * if set, `--gjobs` is simply the number of cores used to process motifs in parallel.\n",
    "    * `--dpi 100`: DPI of plots. Important, since the motif occurrence heatmap is DPI-dependent. (Default: 300)\n",
    "    * `--orientations +,- `: Analyze these orientations of the motifs (Forward, and reverse). (Default: +,+/-, analyze Forward, and non-orientation specific)\n",
    "    * Not specified here:\n",
    "        * ` --margin {INTEGER}`: Number of bases along either side of motif to \"blur\" motif matches for smoothing. (Default: 2)\n",
    "            * It can be useful to set this depending on how strictly your sequences have been centered. If centering on ChIP-seq peak centers, consider a larger margin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mepp_filepath = slopped_cluster_deduplicated_scored_motif_scans_filepath[:-len('.bed')]+f'.for_notebook.mepp'\n",
    "\n",
    "mepp_cmd = (\n",
    "    f'python -m mepp.get_scored_fasta -fi {genome_fa_filepath} '\n",
    "    f'-bed {slopped_cluster_deduplicated_scored_motif_scans_filepath} '\n",
    "    f'|python -m mepp.cli '\n",
    "    f'--fa - '\n",
    "    f'--motifs {motifs_filepath} '\n",
    "    f'--out {mepp_filepath} '\n",
    "    f'--perms 100 '\n",
    "    f'--batch 1000 '\n",
    "    f'--dgt 50 '\n",
    "    f'--jobs 15 '\n",
    "    f'--gjobs 15 '\n",
    "    f'--nogpu '\n",
    "    f'--dpi 100 '\n",
    "    f'--orientations +,- '\n",
    "    f'&> {mepp_filepath}.log'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "run_mepp_cmd = run_cmd = True\n",
    "for cmd in [mepp_cmd]:\n",
    "    print(cmd)\n",
    "    if run_cmd:\n",
    "        ! {cmd}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! tail {mepp_filepath}.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show links to MEPP HTML outputs\n",
    "MEPP outputs HTML files that are useful for visualizing and navigating your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mepp_results_table_fwd_md = f'[Results table, + orientation]({mepp_filepath}/results_table_orientation_fwd.html)'\n",
    "mepp_clustermap_fwd_md = f'[Clustermap, + orientation]({mepp_filepath}/clustermap_orientation_fwd.html)'\n",
    "\n",
    "mepp_results_table_rev_md = f'[Results table, - orientation]({mepp_filepath}/results_table_orientation_rev.html)'\n",
    "mepp_clustermap_rev_md = f'[Clustermap, - orientation]({mepp_filepath}/clustermap_orientation_rev.html)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(mepp_results_table_fwd_md))\n",
    "display(Markdown(mepp_clustermap_fwd_md))\n",
    "display(Markdown(mepp_results_table_rev_md))\n",
    "display(Markdown(mepp_clustermap_rev_md))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example commands for set-based analysis with CentriMo\n",
    "Threshold top and bottom 10% of scored sequences, then use these as positive/negative inputs to set-based MEA, e.g. CentriMo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent = 10\n",
    "lower_percent = percent\n",
    "higher_percent = 100.0-percent\n",
    "lower_thresh, upper_thresh = list(np.percentile(bed_df['Score'], [lower_percent, higher_percent]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_slopped_cluster_deduplicated_scored_motif_scans_filepath = cluster_deduplicated_scored_motif_scans_filepath[:-len('.bed')]+'.upper.bed'\n",
    "lower_slopped_cluster_deduplicated_scored_motif_scans_filepath = cluster_deduplicated_scored_motif_scans_filepath[:-len('.bed')]+'.lower.bed'\n",
    "\n",
    "upper_slopped_cluster_deduplicated_scored_motif_scans_fa_filepath = cluster_deduplicated_scored_motif_scans_filepath[:-len('.bed')]+'.upper.fa'\n",
    "lower_slopped_cluster_deduplicated_scored_motif_scans_fa_filepath = cluster_deduplicated_scored_motif_scans_filepath[:-len('.bed')]+'.lower.fa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_bed_df = bed_df[bed_df['Score']>=upper_thresh].copy()\n",
    "lower_bed_df = bed_df[bed_df['Score']<=lower_thresh].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_bed_df.to_csv(upper_slopped_cluster_deduplicated_scored_motif_scans_filepath, sep = '\\t', index = False, header = None)\n",
    "! head {upper_slopped_cluster_deduplicated_scored_motif_scans_filepath}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_bed_df.to_csv(lower_slopped_cluster_deduplicated_scored_motif_scans_filepath, sep = '\\t', index = False, header = None)\n",
    "! head {lower_slopped_cluster_deduplicated_scored_motif_scans_filepath}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_fa_cmd = ( f'python -m mepp.get_scored_fasta -fi {genome_fa_filepath} '\n",
    "    f'-bed {upper_slopped_cluster_deduplicated_scored_motif_scans_filepath} '\n",
    "    f'> {upper_slopped_cluster_deduplicated_scored_motif_scans_fa_filepath}'\n",
    ")\n",
    "\n",
    "lower_fa_cmd = ( f'python -m mepp.get_scored_fasta -fi {genome_fa_filepath} '\n",
    "    f'-bed {lower_slopped_cluster_deduplicated_scored_motif_scans_filepath} '\n",
    "    f'> {lower_slopped_cluster_deduplicated_scored_motif_scans_fa_filepath}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "run_thresh_fa_cmds = run_cmd = True\n",
    "for cmd in [upper_fa_cmd, lower_fa_cmd]:\n",
    "    print(cmd)\n",
    "    if run_cmd:\n",
    "        ! {cmd}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_bed_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_bed_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example equivalent Centrimo command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meme_motifs_filepath = 'homer.motifs.id_fixed.meme'\n",
    "centrimo_filepath = mepp_filepath[:-len('.mepp')]+f'.upper_vs_lower.for_notebook.centrimo'\n",
    "centrimo_cmd = (\n",
    "    f'mkdir -p {centrimo_filepath} ;'\n",
    "    f'$(which time) --verbose '\n",
    "    f'centrimo --oc {centrimo_filepath} '\n",
    "    f'--neg {lower_slopped_cluster_deduplicated_scored_motif_scans_fa_filepath} '\n",
    "    f'--norc --sep --local --noseq '\n",
    "    f'{upper_slopped_cluster_deduplicated_scored_motif_scans_fa_filepath} '\n",
    "    f'{meme_motifs_filepath}'\n",
    ")\n",
    "print(centrimo_cmd)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
